[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-24ddc0f5d75046c5622901739e7c5dd533143b0c8e959d652212380cedb1ea36.svg)](https://classroom.github.com/a/ke8zCzPd)
[![Open in Codespaces](https://classroom.github.com/assets/launch-codespace-7f7980b617ed060a017424585567c406b6ee15c891e84e1186181d67ecf80aa0.svg)](https://classroom.github.com/open-in-codespaces?assignment_repo_id=13559098)
# Práctica 4: Algoritmos de ordenamiento

## Resumen de los resultados
![image](https://github.com/AGN-Teaching/practica-4-algoritmos-de-ordenamiento-AxelMedrano/assets/125591190/dac7b755-c8eb-400e-8dbe-3ba15e3d1ae9)

Tras haber hecho la tabla con todos los promedios posibles de los tiempos de ejecución y la desviación estándar se puede observar cómo mi laptop logro ejecutar del n = 5 al n = 100 en 0 segundos por lo que toda esa parte está como ceros, y por consiguiente la desviación igual está en 0.
Del n = 500 al n = 10,000 se ve que ni el promedio ni la desviación estándar pasan de un segundo, pero se ve que va aumentando cada vez más.
A partir del n = 50,000 al n = 500,000 (hasta donde pudo mi lap) se puede notar que ya varios promedios pasan de la unidad, sobre todo bubblesort.


## Gráfica del resumen de los resultados
![image](https://github.com/AGN-Teaching/practica-4-algoritmos-de-ordenamiento-AxelMedrano/assets/125591190/044b0d29-f659-4c7f-bad9-2c13e5bb6280)
![image](https://github.com/AGN-Teaching/practica-4-algoritmos-de-ordenamiento-AxelMedrano/assets/125591190/4d13d6d5-9baa-4939-88e5-9f11c8d13320)

Para dejarlo más claro, los números inferiores se refieren a los números n, por lo que en cada uno de esos se ve representado mediante colores cada método, mientras que los números de la izquierda solo hacen referencia a los segundos tardados. El color azul oscuro es el equivalente al insertion sort, el naranja selection sort, el gris bubblesort, el amarillo merge sort y el azul claro el Quicksort, pero estos últimos dos no se pueden apreciar por el tiempo tan corto de estos en la escala total en la primera gráfica, en la segunda que tiene escalas más pequeñas ya se pueden distinguir mejor.

## Análisis de los resultados obtenidos
En el caso de mi laptop tiene un procesador Intel 5 de 2.42GHz, mientras que tiene 8 de ram, esto lo considero importante ya que finalmente depende de cada computadora es que tan rápido se hará por lo que los resultados entre yo y otra persona serán diferentes en mayor o menor cantidad, también al ser al azar los números en sí sería muy complicado que en dos casos de un mismo n saliera lo mismo.
Noté que mi computadora es lo suficientemente rápida como para hacer del n = 5 al n = 100 en 0.0 segundos cerrados, ya que incluso con varios decimales de resultado, estos se mantenían todos en 0, por esto mismo es que en la desviación no se podía ver reflejado ninguna dispersión de los datos, ya que no había nada de lo que dispersarse.
Ya a partir del n = 500 se puede notar que ya le empieza a tomar tiempo a la computadora realizarlo, pero en general siguen siendo tiempos extremadamente pequeños que sí, aunque te fijes por ejemplo que selection sort es más rápido que Bubblesort, realmente no es un tiempo que se pueda considerar significativo como para preferir un método de otro. Esto continúa hasta el n = 10,000.
En el 50,000 se puede notar ya un cambio significativo en los tiempos, ya que por ejemplo los métodos iterativos si pasan directamente de la unidad de segundos mientras que los recursivos se siguen manteniendo con facilidad por debajo del segundo, ya ahí podemos tomar una preferencia hacia los recursivos, sobre todo por Quicksort, que ni una sola vez ha estado por encima de Merge sort en tiempo. En general se puede notar con la desviación que varía mucho de un n a otro, considero que es ya que como se obtienen resultados similares dentro de cada simulación no hay mucha forma de varie demasiado de un resultado a otro y por lo tanto no hay tanta dispersión.
Con los casos de n = 100,000 y 500,000 hay un crecimiento enorme en lo que respecta a tiempos, a pesar de que el segundo caso sea 5 veces mayor al primero hay un crecimiento bastante grande de los segundos, esto es porque, aunque en solo sea mayor en tamaño, requiere mucho más esfuerzo por parte del programa para resolverlos con los diferentes métodos, hay mucha más variabilidad respecto a la ubicación de los números. Lo que note fue que selection sort es por lo general alrededor de 2 veces más grande que insertion sort, mientras que bubble es casi 3 veces mayor que selection sort mientras que los métodos recursivos como lo son merge sort y Quicksort se han mantenido por debajo del segundo incluso a estas alturas, demostrando una efectividad mucho mayor sobre todo Quicksort, resaltando en los arreglos más grandes. Respecto a la desviación en estos casos, como hay un crecimiento enorme de los tiempos también hay mucho más rango de diferencia de tiempos entre un arreglo y otro, por lo que la desviación es más grande en general, excepto con los métodos recursivos que siguen manteniendo tiempos muy pequeños. Los tiempos hechos por los métodos iterativos son tan grandes que se requiere demasiado tiempo para hacerlos.


## Conclusiones
La ausencia de dispersión significativa en los resultados, según se refleja en la desviación estándar, sugiere una consistencia en la ejecución de las simulaciones. Esto podría atribuirse a la naturaleza determinista de los algoritmos y la consistencia en los resultados obtenidos para un mismo conjunto de datos.
La elección de un algoritmo de ordenamiento dependerá del tamaño del conjunto de datos y la importancia de la eficiencia temporal. Los métodos recursivos, en particular Quicksort, demuestran ser preferibles para conjuntos de datos más grandes, mientras que, para conjuntos más pequeños, las diferencias entre los algoritmos pueden no ser tan notorias.
También tengo que resaltar que en mi caso parecía que en el caso de 500,000 iba a tener una duración aproximada de unas 11 horas, y este tiempo se queda corto en comparación a lo que se quería tardar con n´s superiores.  Este tiempo en computadoras personales como la mía ya es mucho y ni hablar de números superiores, que podrían tardar horas en cada m y considerando que son 30 no se ve posible hacer todos los casos en un horario establecido de una semana y menos considerando que una computadora de preferencia no debe de estar tanto tiempo seguido trabajando a menos que sea explícitamente para tenerlo en constante procesamiento.
En resumen, no es algo que se pueda manejar casualmente, si no que se son cosas que se deben de hacer con herramientas hechas para eso. 
